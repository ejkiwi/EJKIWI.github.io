오늘의목표
1.cnn공부 - 합성곱 계층에서의filter, Padding에 대해 더 알아보기.  
2.resnet공부 - Residual Block과 Skip-Connection 에 대해 더 깊이 알아보기

**합성곱 계층에서의 filter**
CNN에서 filter는 커널(n * m의 행렬)와 같은 의미이다. (mask라고도 불린다.)
filter를 사용하는 이유는 사진에서 feature(특징)를 뽑아내기 위함이다.
- 입력 데이터의 전체 이미지에서, filter를 통해 천제 이미지를 순환하며, 특정 filter모양과 일치할수록 더 큰 값을 가지게 될 것인데, 이는 전체 이미지서 특정 filter와 유사한 모양을 가진 부분에 대한 feature들만 얻게 된다는 것을 의미한다. => 특정 filter에 부합하는 feature정보를 얻는 과정.

**Padding**
cnn구조에서, 합성곱층을 지나게 되면, 합성곱 연산으로 인해서 Feature Map의 크기는 입력데이터보다 크기가 작아지게 된다. 이렇게 크기가 작아지는것을 피하기 위해서 Padding 이라는 방법을 사용할 수 있다.
- zero padding : 입력 데이터(이미지) 주위를 0으로 둘러주는 padding의 방법이다.
	![zero padding](https://ejkiwi.github.io/lib/media/zero_Padding.png)
	- P : padding layer의 수
	- n : 이미지의 크기가 n * n
	- f : 커널의 크기(filter의 크기)가  f * f
	- (n+2p) * (n+2p) : 패딩된 이미지의 크기
	- ((n + 2p – f + 1) * (n + 2p – f + 1)) :  합성곱층을 지난 출력 이미지의 크기
- padding이 필요한 이유
	- 이미지 데이터의 축소를 막을 수 있다. -  여러번의 계산을 거쳐야 하는데 초반부터 이미지가 너무 작아져버린다면 학습을 별로 하지 못하고 끝나버릴 수 있기 때문에 padding을 통해 이미지의 크기를 조절해줘야한다.
	- 모서리에 있는 중요한 정보를 충분히 활용할 수 있다. - padding을 사용하지 않는 경우, 모서를 학습할 기회가 적어지게 된다. 만약 중요한 정보가 모서리쪽에 있다면, 모델의 성능이 떨어지기 때문에 padding을 사용하여 모서리의 정보들도 충분히 학습할 수 있도록 해주어야 한다. 
	![패딩과 모서리~](https://ejkiwi.github.io/lib/media/CNN_Padding_Edge.png)
- Valid Padding과 Same Padding : 각각 순서대로 패딩하지 않는 것, 입력데이터와 출력데이터가 동일하도록 하는 패딩을 뜻한다.

**Residual Block** 과 **Skip-Connection**
Residual Block은 층이 깊어지더라도 성능이 뒤떨어지지 않게 하기 위해 제시된 것.
Residual 은 "잔여" 라는 뜻을 가지고 있는데, x를 입력 H(x)를 x의 분포로 가정하면 residual은 최종으로 구하고자 하는 H(x)와 x의 차이로 볼 수 있다.
즉, `Residual = R(x) = H(x) - x` 가 되며 `H(x) = R(x) + x` 로 정리가능하다. 
- ![residualblock]()
- 위 신경망층에서는 F(x)r가 R(x)의 역할을 하기 때문에 Residual Block이라 불리게 된다.
Residual Block은 그레디언트 소실 문제를 약화시키고, 이에 따라 신경망의 깊이가 깊어져도 성능이 떨어지지 않게 되는 것.
- 그레디언트 소실 문제
	- 신경망을 학습시는 과정에서 -> 역전파 알고리즘을 통해 출력층에서 입력층으로 손실함수에 대한 그레디언트를 전파하고, 경사 하강법을 통해 이 그레디언트를 사용하여 각 파라미터를 수정하는 단계를 거치게 됨.
	-  이 때 신경망의 하위층으로 진행될수록 그레디언트가 점점 작아지게 되는 문제가 그레디언트 소실 문제이다.
residual block에서는 x, x+1, x+2 층이 있다고 할 때, x+2층은 x+1층뿐만 아니라 x로부터도 정보를 받을 수 있게 된다. 따라서 역전파 알고리즘이 실행될 때 그레디언트가 작아지는것을 어느정도 막아주는 효과가 발생한다.
이러한 residual block의 방식을 하나의 합성곱층을 기준으로 살펴보았을 때,
한 층의 입력값을 출력값과 합쳐서 다음 층으로 넘겨주는 방식이 그 층의 입력값이 해당 층을 통과하지 않고 다음 층으로 넘어가는 것과 같기 때문에 Skip Connection이라 부르게 되는 것이다.
즉, Residual Block의 핵심은 Skip Connection이라 할 수 있다.