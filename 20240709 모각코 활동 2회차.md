오늘의 목표
1.파이토치 공부하기 - youtube에 있는 파이토치 설명 강좌(https://youtube.com/playlist?list=PLS8gIc2q83Oit-utRso2iblvt00fZOw85&si=i0CZi4e5g_dVJ3dx) 1,2,3강 들으며 공부
2.선배님의 프로젝트 코드 절반 분석하기 - whisper 부분



**파이토치**
al분야에서 google tensorflow와 함께 딥러닝 모델을 구축하고 학습하는 데 가장 많이 사용되고 있는 오픈 소스 기반의 딥러닝 프레임워크임.
- 오픈소스 : 개방형 협업을 장려하는 소프트웨어 개발 모델
- 프레임워크 : 소프트웨어 개발에 있어 하나의 뼈대와 같은 역할을 하는 것으로, 목적에 필요한 것을 고민할 필요 없이 이용할 수 있도록 일괄로 가져다 쓰도록 만들어 놓은 구조화된 틀임.
텐서 : 파이토치의 기본 데이터 타입
- 배열이나 행렬과 유사한 자료 구조이다
- 일반적으로는 1차원 - 벡터 , 2차원 - 행렬, 3차원 이상 - 벡터 이지만, 파이토치에서는 입력과 출력 그리고 학습에 필요한모든 데이터들을 모두 텐서 데이터타입으로 정의하고 있다.
- 텐서의 속성으로는 모양,자료형,저장되는 위치가 있다
- 보통 저장되는 위치는 cpu인데, gpu를 사용할 수 있다면, `.to("cuda")`를 사용해서 텐서를 gpu로 이동시킬 수 있다.
	- gpu : 컴퓨터 그래픽을 처리하는 장치로 그래픽 카드를 구성하는 가장 중요한 핵심 요소.
- 1.파이썬의 리스트 데이터로부터 직접 텐서를 만들 수 있다.
	`- listdata = [[10,20],[30,40]]
	`	tensor1 = torch.Tensor(listdata)`
- 2.파이썬의 넘파이 데이터로부터 직접 텐서를 만들 수도 있다.(넘파이로만들어진건 보통 int로 생성되기때문에 원래 데이터가 float의 형태인 경우, 캐스팅해주는 작업이 필요하기도 하다.)
- 3.파이썬의 랜덤 데이터로부터 직접 텐서를 만들 수도 있다.
	`- tensor3 = torch.rand(2,2)` -> rand()메서드는 0~1사이의 균일 분포 랜덤값을 생성함 ( randn()메서드는 정규분포를 가지는 랜덤값을 생성 )
- 텐서를 넘파이로 바꿀 수도 있다.
	`- tensor.numpy()`
- 인덱싱과 슬라이싱이 가능하다
- elment-wise product 연산 
- matrix multiplication 연산 (행렬곱)
- 텐서를 합칠 수 있다. Tensor Concatenate (dim=0 세로, dim=1 가로)
파이토치 딥러닝 모델 구조 :
	1.데이터정의
		- 기본 데이터타입인 TENSOR로 생성해야함.
		- TensorDataset(x_train,y_train) : 텐서 데이터셋 생성
		- DataLoader(dataset, batch_size, shuffle) : 미니 배치 학습과 데이터 셔플, 멀티 프로세싱 등을 간단하게 수행할 수 있음.
		- 미니 배치 학습 : 전체 데이터를 n등분 하여 각각의 학습 데이터를 배치 방식으로 학습시키는 것.
		- 데이터 셔플 : train데이터와test데이터 간의 동일한 분포를 가지도록 섞어는 것.
		- 멀티 프로세싱 : 여러 작업을 별도의 프로세스를 생성 후 병렬처리를 하는 과정을 거치기 때문에 더 빠르게 결과를 얻을 수 있다.
	2.모델구축
		- nn.Module을 상속받는 class를 생성하여 정의하는 것이 일반적이다.
		- 클래스 속 `__init__`함수에서 계층(신경망 모델을 구성하는)을 정의.
		- 클래스 속 `forward` 함수에서 신경망에 데이터 전달하기를 수하고, 결과값을 리턴함
	3.피드포워드
		- 모델 학습을 위해서는 피드 포워드 계산값과 정답의 차이 계산이 필요  -> 이 계산을 위해서는 손실함수와 옵티마이저가 필요함.
			- 손실함수 : MSE 등
			- 옵티마이저 : SDG, ADAM 
	4.손실함수계산
		- `nn.MSELoss(model(x_train),y_train)` : 피드포워드 계산 값과 정답과의 오차 계산.
		- 이 때, model에 데이터를 전달하면 model 클래스 안에 있는 forward()함수자동으로 forward()함수를 호출하기 때문에 우리가 따로 호출해줄 필요가 없다.
	5.모델학습
		-역전파 코드 : 학습이 진행됨에 따라서 모델 파라미터(가중치와 바이어스)를 업데이트하면서 최적화 시킨다
		- `optimizer.zero.grad()`
		- `loss.backward()`
		- `optimizer.step()`
	- 모델(model) : 각 층을 포함하고 있는 인공신경망 그 자체 (이를 레고처럼 순차적으로 쌓기 -> CNN, RNN 등 다양한 모델 구축 가능) 
	- 3>4>5의 반복 -> 딥러닝 학습
	- 손실함수가 최소가 될 때까지 모델 파라미터(가중치, 바이어스) 값을 찾아감.

**선배님 프로젝트 분석 - whisper**
1.`from faster_whisper import WhisperModel`
2.`def get_whisper() : 
	 3.`   model_size = "medium"  #@param ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3']`
	 4.`   compute_type = "int8"  #@param ['float16', 'int8']
	 5.`   return WhisperModel(model_size, device=DEVICE, cpu_threads=12, compute_type=compute_type).transcribe`

1: faster_whisper 에서 WhisperModel 모듈 불러오기
2: get_whisper 라는 이름의 함수 설정하기
3: model_size는 "medium"이다. model_size가 가질 수 있는 옵션으로는 "tiny","base","small","medium","large","large-v3" 이 있다. -> model_size는 모델의 크기를 뜻한다.
4: compute_type은 "int8"이다. compute_type이 가질 수 있는 옵션으로는 "float16","int8"이 있다. -> compute_type은 계산 유형을 뜻한다.
5: WhisperModel은 4가지의 매개변수를 사용하는데, 여기에서 model_size는 앞서 정한 크기와 같고, device는 모델이 실행될 장치를 지정한다. cpu_threads는 CPU의 스레드 수를 뜻한다. compute_type또한 앞서 정한 계산 유형과 같다. 이 때 .transcribe는 모델의 음성 인식 기능을 호출해서 음성을 텍스트로 변환해준다.